{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\mkrajcovic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os, json, numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "data = pd.read_csv(os.path.join('..', 'data', 'results-raw.csv'))\n",
    "notes = pd.read_csv(os.path.join('..', 'data', 'results-notes.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "answer_columns = list(filter(lambda x: x.startswith(('T', 'pre', 'post')) and not x.endswith('Q'), data.columns))\n",
    "note_columns = list(filter(lambda x: x.endswith('Note'), notes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word counts\n",
    "word_counts = pd.DataFrame(\n",
    "    data={\n",
    "        'id': data.id, \n",
    "        'pre1WordCount': 0, 'pre1F1WordCount': 0, 'pre1F2WordCount': 0, 'pre1F3WordCount': 0,\n",
    "        'pre2WordCount': 0, 'pre2F1WordCount': 0, 'pre2F2WordCount': 0, 'pre2F3WordCount': 0,\n",
    "        'T1Q1WordCount': 0, 'T1Q1F1WordCount': 0, 'T1Q1F2WordCount': 0, 'T1Q1F3WordCount': 0,\n",
    "        'T1Q2WordCount': 0, 'T1Q2F1WordCount': 0, 'T1Q2F2WordCount': 0, 'T1Q2F3WordCount': 0,\n",
    "        'T1Q3WordCount': 0, 'T1Q3F1WordCount': 0, 'T1Q3F2WordCount': 0, 'T1Q3F3WordCount': 0,\n",
    "        'T2Q1WordCount': 0, 'T2Q1F1WordCount': 0, 'T2Q1F2WordCount': 0, 'T2Q1F3WordCount': 0,\n",
    "        'T2Q2WordCount': 0, 'T2Q2F1WordCount': 0, 'T2Q2F2WordCount': 0, 'T2Q2F3WordCount': 0,\n",
    "        'T2Q3WordCount': 0, 'T2Q3F1WordCount': 0, 'T2Q3F2WordCount': 0, 'T2Q3F3WordCount': 0,\n",
    "        'post1WordCount': 0, 'post1F1WordCount': 0, 'post1F2WordCount': 0, 'post1F3WordCount': 0,\n",
    "    }, \n",
    "    index=data.index\n",
    ")\n",
    "\n",
    "# sentiments\n",
    "sentiments = pd.DataFrame(\n",
    "    data={\n",
    "        'id': data.id, \n",
    "        'pre1Sentiment': 0, 'pre1F1Sentiment': 0, 'pre1F2Sentiment': 0, 'pre1F3Sentiment': 0,\n",
    "        'pre2Sentiment': 0, 'pre2F1Sentiment': 0, 'pre2F2Sentiment': 0, 'pre2F3Sentiment': 0,\n",
    "        'T1Q1Sentiment': 0, 'T1Q1F1Sentiment': 0, 'T1Q1F2Sentiment': 0, 'T1Q1F3Sentiment': 0,\n",
    "        'T1Q2Sentiment': 0, 'T1Q2F1Sentiment': 0, 'T1Q2F2Sentiment': 0, 'T1Q2F3Sentiment': 0,\n",
    "        'T1Q3Sentiment': 0, 'T1Q3F1Sentiment': 0, 'T1Q3F2Sentiment': 0, 'T1Q3F3Sentiment': 0,\n",
    "        'T2Q1Sentiment': 0, 'T2Q1F1Sentiment': 0, 'T2Q1F2Sentiment': 0, 'T2Q1F3Sentiment': 0,\n",
    "        'T2Q2Sentiment': 0, 'T2Q2F1Sentiment': 0, 'T2Q2F2Sentiment': 0, 'T2Q2F3Sentiment': 0,\n",
    "        'T2Q3Sentiment': 0, 'T2Q3F1Sentiment': 0, 'T2Q3F2Sentiment': 0, 'T2Q3F3Sentiment': 0,\n",
    "        'post1Sentiment': 0, 'post1F1Sentiment': 0, 'post1F2Sentiment': 0, 'post1F3Sentiment': 0,\n",
    "    }, \n",
    "    index=data.index, dtype='float'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# calculate word counts and sentiment\n",
    "for index, row in data.iterrows():\n",
    "    for column in answer_columns:\n",
    "        count = 0\n",
    "        sentiment = None\n",
    "        if(type(row[column]) == str):\n",
    "            tokens = word_tokenize(row[column])\n",
    "            tokens_no_punct = list(filter(lambda x: x.isalpha(), [word.lower() for word in tokens if word.isalpha()]))\n",
    "            tokens_no_stop = [word for word in tokens_no_punct if not word in stopwords.words()]\n",
    "            count = len(tokens_no_stop)\n",
    "            sentiment = sia.polarity_scores(row[column])['compound']\n",
    "        word_counts.loc[index, column + 'WordCount'] = count\n",
    "        sentiments.loc[index, column + 'Sentiment'] = sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "merged = pd.merge(\n",
    "    notes, word_counts,\n",
    "    left_on='id', right_on='id'\n",
    ")\n",
    "\n",
    "merged = pd.merge(\n",
    "    merged, sentiments,\n",
    "    left_on='id', right_on='id'\n",
    ")\n",
    "\n",
    "merged = pd.merge(\n",
    "    data, merged,\n",
    "    left_on='id', right_on='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "merged.to_csv(os.path.join('..', 'data', 'results.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
